{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4c9ff9-c43e-4d94-ac7a-282b05b6585f",
   "metadata": {},
   "source": [
    "**OBJECTIVE** :\n",
    "WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b0073-b438-45a5-a2db-5f064af653a8",
   "metadata": {},
   "source": [
    "**Model Description:**\n",
    "*Architecture:*\n",
    "  Input Layer (784 neurons) → Takes flattened 28×28 grayscale images.\n",
    "  Hidden Layer 1 (300 neurons) → Sigmoid activation.\n",
    "  Hidden Layer 2 (150 neurons) → Sigmoid activation.\n",
    "  Output Layer (10 neurons) → Produces logits, passed to Softmax for classification.\n",
    "*Hyperparameters:*\n",
    "  Loss Function: Softmax Cross-Entropy\n",
    "  Optimizer: Stochastic Gradient Descent (SGD) \n",
    "  Learning Rate: 0.1\n",
    "  Batch Size: 32\n",
    "  Epochs: 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749f3c3b-9870-48f3-b6de-741f301231e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1279.7699\n",
      "Epoch 2, Loss: 593.3142\n",
      "Epoch 3, Loss: 508.1269\n",
      "Epoch 4, Loss: 447.6935\n",
      "Epoch 5, Loss: 396.6103\n",
      "Epoch 6, Loss: 352.4718\n",
      "Epoch 7, Loss: 315.1519\n",
      "Epoch 8, Loss: 284.7723\n",
      "Epoch 9, Loss: 258.7495\n",
      "Epoch 10, Loss: 236.2349\n",
      "Epoch 11, Loss: 217.0366\n",
      "Epoch 12, Loss: 199.7343\n",
      "Epoch 13, Loss: 185.6813\n",
      "Epoch 14, Loss: 171.9853\n",
      "Epoch 15, Loss: 161.2843\n",
      "Epoch 16, Loss: 149.1017\n",
      "Epoch 17, Loss: 139.6993\n",
      "Epoch 18, Loss: 130.0861\n",
      "Epoch 19, Loss: 122.5064\n",
      "Epoch 20, Loss: 114.1880\n",
      "Epoch 21, Loss: 107.6347\n",
      "Epoch 22, Loss: 100.6482\n",
      "Epoch 23, Loss: 95.1949\n",
      "Epoch 24, Loss: 89.5251\n",
      "Epoch 25, Loss: 83.8410\n",
      "Epoch 26, Loss: 78.9405\n",
      "Epoch 27, Loss: 74.4654\n",
      "Epoch 28, Loss: 69.9460\n",
      "Epoch 29, Loss: 65.4590\n",
      "Epoch 30, Loss: 61.7510\n",
      "Epoch 31, Loss: 58.0726\n",
      "Epoch 32, Loss: 54.0214\n",
      "Epoch 33, Loss: 51.3869\n",
      "Epoch 34, Loss: 47.9156\n",
      "Epoch 35, Loss: 45.1794\n",
      "Epoch 36, Loss: 42.3969\n",
      "Epoch 37, Loss: 40.1871\n",
      "Epoch 38, Loss: 37.8948\n",
      "Epoch 39, Loss: 35.4205\n",
      "Epoch 40, Loss: 33.4166\n",
      "Epoch 41, Loss: 30.9513\n",
      "Epoch 42, Loss: 29.2728\n",
      "Epoch 43, Loss: 27.9136\n",
      "Epoch 44, Loss: 26.0168\n",
      "Epoch 45, Loss: 24.5491\n",
      "Epoch 46, Loss: 23.0350\n",
      "Epoch 47, Loss: 21.9691\n",
      "Epoch 48, Loss: 20.7057\n",
      "Epoch 49, Loss: 19.2882\n",
      "Epoch 50, Loss: 18.2032\n",
      "Epoch 51, Loss: 17.2233\n",
      "Epoch 52, Loss: 16.3939\n",
      "Epoch 53, Loss: 15.7044\n",
      "Epoch 54, Loss: 14.6801\n",
      "Epoch 55, Loss: 13.9655\n",
      "Epoch 56, Loss: 13.1253\n",
      "Epoch 57, Loss: 12.4978\n",
      "Epoch 58, Loss: 12.0011\n",
      "Epoch 59, Loss: 11.2239\n",
      "Epoch 60, Loss: 10.7556\n",
      "Epoch 61, Loss: 10.3508\n",
      "Epoch 62, Loss: 9.8008\n",
      "Epoch 63, Loss: 9.4152\n",
      "Epoch 64, Loss: 8.8530\n",
      "Epoch 65, Loss: 8.5543\n",
      "Epoch 66, Loss: 8.2675\n",
      "Epoch 67, Loss: 7.8554\n",
      "Epoch 68, Loss: 7.6153\n",
      "Epoch 69, Loss: 7.2070\n",
      "Epoch 70, Loss: 6.9896\n",
      "Epoch 71, Loss: 6.6849\n",
      "Epoch 72, Loss: 6.4241\n",
      "Epoch 73, Loss: 6.2363\n",
      "Epoch 74, Loss: 6.0179\n",
      "Epoch 75, Loss: 5.7929\n",
      "Epoch 76, Loss: 5.5845\n",
      "Epoch 77, Loss: 5.4290\n",
      "Epoch 78, Loss: 5.1985\n",
      "Epoch 79, Loss: 5.0978\n",
      "Epoch 80, Loss: 4.9139\n",
      "Epoch 81, Loss: 4.8382\n",
      "Epoch 82, Loss: 4.6481\n",
      "Epoch 83, Loss: 4.5307\n",
      "Epoch 84, Loss: 4.3853\n",
      "Epoch 85, Loss: 4.2529\n",
      "Epoch 86, Loss: 4.1666\n",
      "Epoch 87, Loss: 4.0936\n",
      "Epoch 88, Loss: 3.9802\n",
      "Epoch 89, Loss: 3.8452\n",
      "Epoch 90, Loss: 3.7640\n",
      "Epoch 91, Loss: 3.6582\n",
      "Epoch 92, Loss: 3.5533\n",
      "Epoch 93, Loss: 3.5118\n",
      "Epoch 94, Loss: 3.4464\n",
      "Epoch 95, Loss: 3.3534\n",
      "Epoch 96, Loss: 3.2665\n",
      "Epoch 97, Loss: 3.1948\n",
      "Epoch 98, Loss: 3.1361\n",
      "Epoch 99, Loss: 3.0526\n",
      "Epoch 100, Loss: 2.9900\n",
      "Final Training Accuracy (sgd): 1.0000\n",
      "Final Test Accuracy (Asgd): 0.9811\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize (0 to 1)\n",
    "    image = tf.reshape(image, [-1])  # Flatten (28x28 → 784)\n",
    "    label = tf.one_hot(label, depth=10)  # One-hot encode labels\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 784\n",
    "hidden_dim1 = 300\n",
    "hidden_dim2 = 150\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "# Define forward pass\n",
    "def model(x):\n",
    "    hidden_layer1 = tf.sigmoid(tf.matmul(x, W1) + b1)  # First Hidden Layer\n",
    "    hidden_layer2 = tf.sigmoid(tf.matmul(hidden_layer1, W2) + b2)  # Second Hidden Layer\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3  # Output layer (logits)\n",
    "    return logits\n",
    "\n",
    "# Loss function (Softmax Cross-Entropy)\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy function\n",
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n",
    "# Optimizer (sgd)\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "\n",
    "# Training step function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Compute final training and test accuracy\n",
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy (sgd): {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy (sgd): {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d585a29-8712-4c39-9a05-8a2d3e3339d9",
   "metadata": {},
   "source": [
    "**CODE DESCRIPTION:**\n",
    "1. *Dataset Loading & Preprocessing:*\n",
    "   -The MNIST dataset is loaded using tensorflow_datasets.\n",
    "   -Images are normalized (values scaled between 0 and 1) for better training stability.\n",
    "   -Labels are one-hot encoded \n",
    "   -The dataset is shuffled and batched to improve training efficiency.\n",
    "2. *Model Definition:*\n",
    "   -Weights (W) and biases (b) are initialized randomly for each layer.\n",
    "   -First hidden layer: Uses sigmoid activation on W1 * input + b1.\n",
    "   -Second hidden layer: Uses sigmoid activation on W2 * hidden_layer1 + b2.\n",
    "   -The final output is logits (raw scores), which will later be converted into probabilities using softmax.\n",
    "4. *Loss Function:*\n",
    "   -The loss function is Softmax Cross-Entropy, which measures how far the predicted probability distribution is from the actual label distribution.\n",
    "5. *Accuracy Computation:*\n",
    "   -Converts logits into class predictions using tf.argmax.\n",
    "   -Compares them with actual labels and calculates the percentage of correct predictions.\n",
    "6. *Optimization (SGD ):*\n",
    "   -Uses Stochastic Gradient Descent (SGD) with a learning rate of 0.1.\n",
    "   -This optimizer updates weights and biases based on computed gradients from the loss function.\n",
    "7. *Training Loop:*\n",
    "   -Runs for 100 epochs, where each epoch loops through the entire training dataset.\n",
    "   -Computes total loss for each epoch and prints the loss value.\n",
    "   -At the end of training, accuracy is computed for both the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad2935-9897-46b0-9fb5-6ff1040dc2f6",
   "metadata": {},
   "source": [
    "**Training Accuracy:** 1.0000\n",
    "**Test Accuracy:** 0.9811"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937370b-a69a-41b7-b473-f83aea176e84",
   "metadata": {},
   "source": [
    "**MY COMMENTS:**\n",
    "-SGD with momentum provides better accuracy when compared to SGD without momentum\n",
    "-ADAM is faster compared to SGD \n",
    "-ReLu activation function is better option compared to Sigmoid.\n",
    "-SGD takes more time compared to both sgd with momentum and adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82713e09-363f-481f-abb2-a06471109fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
