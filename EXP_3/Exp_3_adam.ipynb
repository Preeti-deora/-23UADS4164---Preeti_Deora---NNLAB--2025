{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103810c2-04b1-417a-b8ad-df74a870203f",
   "metadata": {},
   "source": [
    "**OBJECTIVE** :\n",
    "WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a600e4-5c3f-4095-9127-6be63bab67ea",
   "metadata": {},
   "source": [
    "**Model Description:**\n",
    "*Architecture:*\n",
    "  Input Layer (784 neurons) → Takes flattened 28×28 grayscale images.\n",
    "  Hidden Layer 1 (128 neurons) → Sigmoid activation.\n",
    "  Hidden Layer 2 (64 neurons) → Sigmoid activation.\n",
    "  Output Layer (10 neurons) → Produces logits, passed to Softmax for classification.\n",
    "*Hyperparameters:*\n",
    "  Loss Function: Softmax Cross-Entropy\n",
    "  Optimizer: Adam\n",
    "  Learning Rate: 0.01\n",
    "  Batch Size: 100\n",
    "  Epochs: 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73058764-8bdd-4336-9638-30edf1869402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 173.6847\n",
      "Epoch 2, Loss: 65.5254\n",
      "Epoch 3, Loss: 50.9543\n",
      "Epoch 4, Loss: 39.6772\n",
      "Epoch 5, Loss: 33.9670\n",
      "Epoch 6, Loss: 30.5116\n",
      "Epoch 7, Loss: 26.5907\n",
      "Epoch 8, Loss: 24.5352\n",
      "Epoch 9, Loss: 24.4435\n",
      "Epoch 10, Loss: 22.0982\n",
      "Final Training Accuracy (Adam): 0.9906\n",
      "Final Test Accuracy (Adam): 0.9738\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize (0 to 1)\n",
    "    image = tf.reshape(image, [-1])  # Flatten (28x28 → 784)\n",
    "    label = tf.one_hot(label, depth=10)  # One-hot encode labels\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 784\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "# Define forward pass\n",
    "def model(x):\n",
    "    hidden_layer1 = tf.sigmoid(tf.matmul(x, W1) + b1)  # First Hidden Layer\n",
    "    hidden_layer2 = tf.sigmoid(tf.matmul(hidden_layer1, W2) + b2)  # Second Hidden Layer\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3  # Output layer (logits)\n",
    "    return logits\n",
    "\n",
    "# Loss function (Softmax Cross-Entropy)\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy function\n",
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n",
    "# Optimizer (Adam)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# Training step function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Compute final training and test accuracy\n",
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy (Adam): {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy (Adam): {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a500c4-e55b-4991-a930-1c8a1cadca3e",
   "metadata": {},
   "source": [
    "**CODE DESCRIPTION:**\n",
    "1. *Dataset Loading & Preprocessing:*\n",
    "   -The MNIST dataset is loaded using tensorflow_datasets.\n",
    "   -Images are normalized (values scaled between 0 and 1) for better training stability.\n",
    "   -Labels are one-hot encoded \n",
    "   -The dataset is shuffled and batched to improve training efficiency.\n",
    "   -Each image is flattened from 28×28 to a 1D vector of 784 values.\n",
    "3. *Model Definition:*\n",
    "   -Weights (W) and biases (b) are initialized randomly for each layer.\n",
    "   -The forward pass involves matrix multiplications and sigmoid activation in the hidden layers.\n",
    "   -The output layer produces logits (raw scores before applying softmax).\n",
    "4. *Loss Function:*\n",
    "   -The loss function is Softmax Cross-Entropy, which measures how far the predicted probability distribution is from the actual label distribution.\n",
    "5. *Accuracy Computation:*\n",
    "   -Accuracy is computed by checking how many predicted labels match the actual labels.\n",
    "6. *Optimization (Adam Optimizer):*\n",
    "   -Adam optimizer is used to update weights. It adapts the learning rate dynamically to improve training speed and convergence.\n",
    "7. *Training Loop:*\n",
    "   -The model is trained for 10 epochs.\n",
    "   -In each epoch, all training samples are passed through the model, and the weights are updated based on the computed loss.\n",
    "   -At the end of training, accuracy is computed for both the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79371a20-d09d-40ff-99a8-9f198a965fd8",
   "metadata": {},
   "source": [
    "**MY COMMENTS:**\n",
    "-Adam optimizer's learning rate must be low.\n",
    "-Use ReLU instead of Sigmoid.\n",
    "-It works better with greater batch size\n",
    "-Faster Compared to SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439aaba1-61a2-4012-bdb4-bafb145865bc",
   "metadata": {},
   "source": [
    "**Training Accuracy:** 0.9906\n",
    "**Test Accuracy:** 0.9738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609925a-1680-4364-b939-17f5a3eb2de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
