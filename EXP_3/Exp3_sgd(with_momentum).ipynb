{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d97466-8777-4f3f-b808-3f3131f1828d",
   "metadata": {},
   "source": [
    "**OBJECTIVE** :\n",
    "WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18dda0b-56b7-4425-af10-349b394ab0e2",
   "metadata": {},
   "source": [
    "**Model Description:**\n",
    "*Architecture:*\n",
    "  Input Layer (784 neurons) → Takes flattened 28×28 grayscale images.\n",
    "  Hidden Layer 1 (128 neurons) → Sigmoid activation.\n",
    "  Hidden Layer 2 (64 neurons) → Sigmoid activation.\n",
    "  Output Layer (10 neurons) → Produces logits, passed to Softmax for classification.\n",
    "*Hyperparameters:*\n",
    "  Loss Function: Softmax Cross-Entropy\n",
    "  Optimizer: Stochastic Gradient Descent (SGD) with Momentum (0.9)\n",
    "  Learning Rate: 0.1\n",
    "  Batch Size: 32\n",
    "  Epochs: 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3517dc7-a1de-4cc2-a822-5000b8b60c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 671.7911\n",
      "Epoch 2, Loss: 248.4321\n",
      "Epoch 3, Loss: 169.9272\n",
      "Epoch 4, Loss: 133.2994\n",
      "Epoch 5, Loss: 99.0014\n",
      "Epoch 6, Loss: 80.7056\n",
      "Epoch 7, Loss: 63.2901\n",
      "Epoch 8, Loss: 50.5881\n",
      "Epoch 9, Loss: 39.6281\n",
      "Epoch 10, Loss: 35.4824\n",
      "Final Training Accuracy (SGD): 0.9970\n",
      "Final Test Accuracy (SGD): 0.9801\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "dataset, info = tfds.load(\"mnist\", as_supervised=True, with_info=True)\n",
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# Define batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize (0 to 1)\n",
    "    image = tf.reshape(image, [-1])  # Flatten (28x28 → 784)\n",
    "    label = tf.one_hot(label, depth=10)  # One-hot encode labels\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(10000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE)\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = 784\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = tf.Variable(tf.random.normal([input_dim, hidden_dim1], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([hidden_dim1]))\n",
    "W2 = tf.Variable(tf.random.normal([hidden_dim1, hidden_dim2], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([hidden_dim2]))\n",
    "W3 = tf.Variable(tf.random.normal([hidden_dim2, output_dim], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "# Define forward pass\n",
    "def model(x):\n",
    "    hidden_layer1 = tf.sigmoid(tf.matmul(x, W1) + b1)  # First Hidden Layer\n",
    "    hidden_layer2 = tf.sigmoid(tf.matmul(hidden_layer1, W2) + b2)  # Second Hidden Layer\n",
    "    logits = tf.matmul(hidden_layer2, W3) + b3  # Output layer (logits)\n",
    "    return logits\n",
    "\n",
    "# Loss function (Softmax Cross-Entropy)\n",
    "def compute_loss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "# Accuracy function\n",
    "def compute_accuracy(dataset):\n",
    "    correct_preds, total_samples = 0, 0\n",
    "    for images, labels in dataset:\n",
    "        logits = model(images)\n",
    "        correct_preds += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, axis=1), tf.argmax(labels, axis=1)), tf.float32)).numpy()\n",
    "        total_samples += images.shape[0]\n",
    "    return correct_preds / total_samples\n",
    "\n",
    "# Optimizer (SGD with Momentum)\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "# Training step function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(images)\n",
    "        loss = compute_loss(logits, labels)\n",
    "    gradients = tape.gradient(loss, [W1, b1, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_dataset:\n",
    "        loss = train_step(images, labels)\n",
    "        total_loss += loss.numpy()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Compute final training and test accuracy\n",
    "train_accuracy = compute_accuracy(train_dataset)\n",
    "test_accuracy = compute_accuracy(test_dataset)\n",
    "\n",
    "print(f\"Final Training Accuracy (SGD): {train_accuracy:.4f}\")\n",
    "print(f\"Final Test Accuracy (SGD): {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceed654-d13b-4796-9ec7-3c1df6b1937d",
   "metadata": {},
   "source": [
    "**CODE DESCRIPTION:**\n",
    "1. *Dataset Loading & Preprocessing:*\n",
    "   -The MNIST dataset is loaded using tensorflow_datasets.\n",
    "   -Images are normalized (values scaled between 0 and 1) for better training stability.\n",
    "   -Labels are one-hot encoded \n",
    "   -The dataset is shuffled and batched to improve training efficiency.\n",
    "2. *Model Definition:*\n",
    "   -Weights (W) and biases (b) are initialized randomly for each layer.\n",
    "   -The model performs matrix multiplications followed by the sigmoid activation function in each hidden layer.\n",
    "   -The final output is logits (raw scores), which will later be converted into probabilities using softmax.\n",
    "3. *Loss Function:*\n",
    "   -The loss function is Softmax Cross-Entropy, which measures how far the predicted probability distribution is from the actual label distribution.\n",
    "4. *Accuracy Computation:*\n",
    "   -Accuracy is computed by checking how many predicted labels match the actual labels.\n",
    "5. *Optimization (SGD with Momentum):*\n",
    "   -SGD (Stochastic Gradient Descent) with Momentum is used to update weights.\n",
    "   -Momentum helps the optimizer move past local minima and speeds up convergence.\n",
    "6. *Training Loop:*\n",
    "   -The model is trained for 10 epochs.\n",
    "   -In each epoch, all training samples are passed through the model, and the weights are updated based on the computed loss.\n",
    "   -At the end of training, accuracy is computed for both the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08525d0-721b-437a-af40-dfe28b1ddfd3",
   "metadata": {},
   "source": [
    "**Training Accuracy:** 0.9970\n",
    "**Test Accuracy:** 0.9801"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770cc07-aef7-4441-9435-613d42a4f85c",
   "metadata": {},
   "source": [
    "**MY COMMENTS:**\n",
    "-SGD with momentum provides better accuracy when compared to SGD without momentum\n",
    "-ADAM is faster compared to SGD \n",
    "-ReLu activation function is better option compared to Sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77461e6-2795-4507-82cf-547a1f30f194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
