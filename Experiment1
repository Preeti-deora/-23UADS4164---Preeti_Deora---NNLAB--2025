import numpy as np

class Perceptron:
    def __init__(self, input_size, learning_rate=0.01, epochs=100):
        self.weights = np.zeros(input_size + 1)  # +1 for bias
        self.learning_rate = learning_rate
        self.epochs = epochs

    def activation_function(self, x):
        return 1 if x >= 0 else 0

    def predict(self, x):
        # Add bias term
        x = np.insert(x, 0, 1)
        linear_output = np.dot(self.weights, x)
        return self.activation_function(linear_output)

    def train(self, X, y):
        X = np.array(X)
        y = np.array(y)

        # Add bias term to input
        X = np.c_[np.ones(X.shape[0]), X]

        for epoch in range(self.epochs):
            for i in range(X.shape[0]):
                prediction = self.activation_function(np.dot(self.weights, X[i]))
                self.weights += self.learning_rate * (y[i] - prediction) * X[i]

    def accuracy(self, X, y):
        correct_predictions = 0
        for i in range(len(X)):
            if self.predict(X[i]) == y[i]:
                correct_predictions += 1
        return correct_predictions / len(X)

# Example usage
if __name__ == "__main__":
    # Example dataset (AND gate)
    X = [
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
    ]
    y = [0, 0, 0, 1]  # AND gate output

    perceptron = Perceptron(input_size=2, learning_rate=0.1, epochs=10)
    perceptron.train(X, y)

    print("Trained weights:", perceptron.weights)

    # Test the perceptron
    for sample in X:
        print(f"Input: {sample}, Predicted: {perceptron.predict(sample)}")

    # Calculate accuracy
    accuracy = perceptron.accuracy(X, y)
    print(f"Accuracy: {accuracy * 100:.2f}%")
