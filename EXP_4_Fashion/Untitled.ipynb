{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4dc81a-9c7f-49ff-b092-47c8bc6ffe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "GPU not found. Using CPU instead.\n",
      "Batch Size: 1000, Epochs: 50\n",
      "Test Accuracy: 0.8198\n",
      "Execution Time: 34.45 seconds\n",
      "\n",
      "Results saved to fashion_mnist_nn_results_1000.xlsx\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from openpyxl import Workbook, load_workbook\n",
    "from openpyxl.drawing.image import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU not found. Using CPU instead.')\n",
    "\n",
    "# Load Fashion MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = tf.convert_to_tensor(x_train.reshape(-1, 784).astype(np.float32) / 255.0)\n",
    "x_test = tf.convert_to_tensor(x_test.reshape(-1, 784).astype(np.float32) / 255.0)\n",
    "\n",
    "y_train = tf.one_hot(y_train, depth=10, dtype=tf.float32)\n",
    "y_test_labels = tf.convert_to_tensor(y_test)  # Original labels for confusion matrix\n",
    "y_test = tf.one_hot(y_test, depth=10, dtype=tf.float32)\n",
    "\n",
    "# Model parameters\n",
    "n_hidden = 256\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([784, n_hidden], stddev=0.1, dtype=tf.float32))\n",
    "b1 = tf.Variable(tf.random.normal([n_hidden], stddev=0.1, dtype=tf.float32))\n",
    "W2 = tf.Variable(tf.random.normal([n_hidden, 10], stddev=0.1, dtype=tf.float32))\n",
    "b2 = tf.Variable(tf.random.normal([10], stddev=0.1, dtype=tf.float32))\n",
    "\n",
    "# Define model\n",
    "@tf.function\n",
    "def model(X):\n",
    "    Z1 = tf.add(tf.matmul(X, W1), b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(A1, W2), b2)\n",
    "    return Z2\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch_x)\n",
    "        loss = loss_fn(batch_y, logits)\n",
    "    grads = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "    optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2]))\n",
    "    return loss\n",
    "\n",
    "# Configurations for batch size and epochs\n",
    "configs = [(1000, 50)]\n",
    "\n",
    "# Excel file setup\n",
    "output_file = \"fashion_mnist_nn_results_1000.xlsx\"\n",
    "\n",
    "# If file exists, load; else, create a new one\n",
    "if os.path.exists(output_file):\n",
    "    try:\n",
    "        wb = load_workbook(output_file)\n",
    "        ws = wb.active\n",
    "    except:\n",
    "        os.remove(output_file)  # Delete corrupted file\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.title = \"Results\"\n",
    "        ws.append([\"Batch Size\", \"Epochs\", \"Train Loss\", \"Train Accuracy\", \"Test Accuracy\", \"Execution Time\"])\n",
    "        wb.save(output_file)\n",
    "else:\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"Results\"\n",
    "    ws.append([\"Batch Size\", \"Epochs\", \"Train Loss\", \"Train Accuracy\", \"Test Accuracy\", \"Execution Time\"])\n",
    "    wb.save(output_file)\n",
    "\n",
    "# Run training\n",
    "with tf.device('/GPU:0'):\n",
    "    for batch_size, epochs in configs:\n",
    "        start_time = time.time()\n",
    "        loss_curve, acc_curve = [], []\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size).shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_x, batch_y in dataset:\n",
    "                train_loss = train_step(batch_x, batch_y)\n",
    "\n",
    "            # Compute train accuracy\n",
    "            train_logits = model(x_train)\n",
    "            train_acc = tf.reduce_mean(tf.cast(tf.argmax(train_logits, axis=1) == tf.argmax(y_train, axis=1), tf.float32))\n",
    "            loss_curve.append(train_loss.numpy())\n",
    "            acc_curve.append(train_acc.numpy())\n",
    "\n",
    "        # Compute test accuracy\n",
    "        test_logits = model(x_test)\n",
    "        test_acc = tf.reduce_mean(tf.cast(tf.argmax(test_logits, axis=1) == tf.argmax(y_test, axis=1), tf.float32))\n",
    "        y_pred = tf.argmax(test_logits, axis=1)\n",
    "        conf_matrix = confusion_matrix(y_test_labels.numpy(), y_pred.numpy())\n",
    "        exec_time = time.time() - start_time\n",
    "\n",
    "        # Append results to Excel\n",
    "        ws.append([batch_size, epochs, train_loss.numpy(), train_acc.numpy(), test_acc.numpy(), exec_time])\n",
    "\n",
    "        # Save intermediate results\n",
    "        wb.save(output_file)\n",
    "\n",
    "        # Plot Loss and Accuracy Curves\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(epochs), loss_curve, label='Loss', color='red')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Curve (Batch {batch_size}, Epochs {epochs})')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(epochs), acc_curve, label='Accuracy', color='blue')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Accuracy Curve (Batch {batch_size}, Epochs {epochs})')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"fashion_mnist_curves_{batch_size}_{epochs}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Save confusion matrix\n",
    "        df_conf_matrix = pd.DataFrame(conf_matrix)\n",
    "        with pd.ExcelWriter(output_file, mode='a', if_sheet_exists=\"new\") as writer:\n",
    "            df_conf_matrix.to_excel(writer, sheet_name=f\"Conf_Matrix_{batch_size}_{epochs}\")\n",
    "\n",
    "        # Add accuracy/loss curve image to Excel\n",
    "        ws_image = wb.create_sheet(title=f\"Curve_{batch_size}_{epochs}\")\n",
    "        img = Image(f\"fashion_mnist_curves_{batch_size}_{epochs}.png\")\n",
    "        ws_image.add_image(img, \"A1\")\n",
    "\n",
    "        print(f\"Batch Size: {batch_size}, Epochs: {epochs}\")\n",
    "        print(f\"Test Accuracy: {test_acc.numpy():.4f}\")\n",
    "        print(f\"Execution Time: {exec_time:.2f} seconds\\n\")\n",
    "\n",
    "    # Final Save\n",
    "    wb.save(output_file)\n",
    "    wb.close()\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b562c9-c06a-4957-acbf-50f95224bda4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
