{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e645e9e-7b24-4c9a-b372-ed88d204fc73",
   "metadata": {},
   "source": [
    "**OBJECTIVE**:\n",
    "WAP to evaluate the performance of implemented three-layer neural network with variations in activation functions, size of hidden layer, learnin  rate, batch size and \n",
    "number of epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc33749-7d0b-402b-b37f-437e13e3d14a",
   "metadata": {},
   "source": [
    "**Description of the Model:**\n",
    "This is a basic neural network with one hidden layer, meaning it has:\n",
    " 1. Input Layer: Takes the flattened image (28×28 pixels = 784 values).\n",
    " 2. Hidden Layer: Contains 256 neurons that learn patterns from the images.\n",
    "      -Uses ReLU activation (which helps the model learn complex patterns).\n",
    " 3. Output Layer: Contains 10 neurons, one for each digit (0-9).\n",
    " 4. Uses softmax logits, meaning it outputs probabilities for each digit.\n",
    "\n",
    "\n",
    "*The training process involves*:\n",
    "1. Calculating loss (cross-entropy loss) to measure how well the model is performing.\n",
    "2. Using the Adam optimizer to update weights and improve predictions.\n",
    "3. Running training over multiple epochs (iterations over the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d39d6b-acc4-4d3d-b934-157d5a8c7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU not found. Using CPU instead.')\n",
    "\n",
    "# Load MNIST dataset (converted to tensors)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = tf.convert_to_tensor(x_train.reshape(-1, 784).astype(np.float32) / 255.0)\n",
    "x_test = tf.convert_to_tensor(x_test.reshape(-1, 784).astype(np.float32) / 255.0)\n",
    "\n",
    "y_train = tf.one_hot(y_train, depth=10, dtype=tf.float32)\n",
    "y_test_labels = tf.convert_to_tensor(y_test)  # Keep original labels for confusion matrix\n",
    "y_test = tf.one_hot(y_test, depth=10, dtype=tf.float32)\n",
    "\n",
    "# Define model parameters using random initialization\n",
    "n_hidden = 256\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal([784, n_hidden], mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "b1 = tf.Variable(tf.random.normal([n_hidden], mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "W2 = tf.Variable(tf.random.normal([n_hidden, 10], mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "b2 = tf.Variable(tf.random.normal([10], mean=0.0, stddev=0.1, dtype=tf.float32))\n",
    "\n",
    "# Define model using pure tensors\n",
    "@tf.function\n",
    "def model(X):\n",
    "    Z1 = tf.add(tf.matmul(X, W1), b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(A1, W2), b2)\n",
    "    return Z2\n",
    "\n",
    "# Loss function using tensors\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Training step using tensors\n",
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(batch_x)\n",
    "        loss = loss_fn(batch_y, logits)\n",
    "    grads = tape.gradient(loss, [W1, b1, W2, b2])\n",
    "    optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2]))\n",
    "    return loss\n",
    "\n",
    "# Hyperparameter tuning configurations\n",
    "configs = [(10, 100), (10, 50), (10, 10), (100, 100), (100, 50), (100, 10), (1, 100), (1, 50), (1, 10)]\n",
    "\n",
    "results = []\n",
    "output_file = \"mnist_nn_results.csv\"\n",
    "\n",
    "# Run on GPU\n",
    "with tf.device('/GPU:0'):\n",
    "    for batch_size, epochs in configs:\n",
    "        start_time = time.time()\n",
    "        loss_curve, acc_curve = [], []\n",
    "\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size).shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_x, batch_y in dataset:\n",
    "                train_loss = train_step(batch_x, batch_y)\n",
    "\n",
    "            # Compute train accuracy using pure tensors\n",
    "            train_logits = model(x_train)\n",
    "            train_acc = tf.reduce_mean(tf.cast(tf.argmax(train_logits, axis=1) == tf.argmax(y_train, axis=1), tf.float32))\n",
    "            loss_curve.append(train_loss.numpy())\n",
    "            acc_curve.append(train_acc.numpy())\n",
    "\n",
    "        # Compute test accuracy using pure tensors\n",
    "        test_logits = model(x_test)\n",
    "        test_acc = tf.reduce_mean(tf.cast(tf.argmax(test_logits, axis=1) == tf.argmax(y_test, axis=1), tf.float32))\n",
    "        y_pred = tf.argmax(test_logits, axis=1)\n",
    "        conf_matrix = confusion_matrix(y_test_labels.numpy(), y_pred.numpy())\n",
    "        exec_time = time.time() - start_time\n",
    "\n",
    "        # Store results\n",
    "        results.append([batch_size, epochs, train_loss.numpy(), train_acc.numpy(), test_acc.numpy(), exec_time])\n",
    "\n",
    "        # Print confusion matrix\n",
    "        print(f\"Confusion Matrix for Batch Size {batch_size}, Epochs {epochs}:\\n\", conf_matrix)\n",
    "\n",
    "        # Plot loss and accuracy curves\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(epochs), loss_curve, label='Loss', color='red')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss Curve (Batch Size {batch_size}, Epochs {epochs})')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(epochs), acc_curve, label='Accuracy', color='blue')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Accuracy Curve (Batch Size {batch_size}, Epochs {epochs})')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # Print results\n",
    "        print(f\"Batch Size: {batch_size}, Epochs: {epochs}\")\n",
    "        print(f\"Test Accuracy: {test_acc.numpy():.4f}\")\n",
    "        print(f\"Execution Time: {exec_time:.2f} seconds\\n\")\n",
    "\n",
    "    # Save results to file\n",
    "    df = pd.DataFrame(results, columns=[\"Batch Size\", \"Epochs\", \"Train Loss\", \"Train Accuracy\", \"Test Accuracy\", \"Execution Time\"])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d2e06-63bf-40aa-acea-a7ed21b041c7",
   "metadata": {},
   "source": [
    " **Description of code:**\n",
    "1. Loading the MNIST Dataset\n",
    "   11. The MNIST dataset contains 60,000 training images and 10,000 test images of handwritten digits.\n",
    "   12. Images are grayscale (0-255 pixel values), so they are normalized (converted to values between 0 and 1).\n",
    "   13. Labels (digits 0-9) are converted into one-hot encoded format (e.g., digit 3 → [0,0,0,1,0,0,0,0,0,0]).\n",
    "\n",
    "2. Defining the Neural Network\n",
    "   21. Weights and biases are created using tf.random.normal(), which gives random initial values.\n",
    "   22. The model is implemented manually (without using Keras layers).\n",
    "   23. Forward propagation is done using matrix multiplication (tf.matmul()) and activation functions (tf.nn.relu()).\n",
    "\n",
    "3. Training Process\n",
    "   31. The model is trained using a batch size (small sets of images are processed at a time).\n",
    "   32. For each batch:\n",
    "       -->Predictions are made using forward propagation.\n",
    "       -->Loss is calculated (how far the predictions are from actual labels).\n",
    "       -->Gradients are computed using tf.GradientTape() to update the weights.\n",
    "       -->Weights and biases are updated using the Adam optimizer.\n",
    "       -->Training repeats for multiple epochs (full passes through the dataset).\n",
    "\n",
    "4. Performance Evaluation\n",
    "   41. After training, the model is tested on unseen images.\n",
    "   42. Accuracy is calculated by checking how many predictions match the correct labels.\n",
    "   43. A confusion matrix is created to see which digits are misclassified.\n",
    "\n",
    "5. Saving Results\n",
    "   51. The results for different batch sizes and epochs are saved in a CSV file (mnist_nn_results.csv) for later analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759fcd4-a205-4b24-9c0a-baf65c72f1e7",
   "metadata": {},
   "source": [
    "**MY COMMENTS:**\n",
    "*HIDDEN LAYER:*\n",
    "-->The model has one hidden layer with 256 neurons, which is a good choice but adding one more layer might improve performance a bit.\n",
    "\n",
    "*BATCH SIZE:*\n",
    "-->Batch size 1 is too slow, the model learns very slowly.\n",
    "-->Batch size 10 is also slow, but better than Batch size 1.\n",
    "-->Batch size 100 is the best choice, it makes training fast while keeping accuracy good.\n",
    "\n",
    "*EPOCHS:*\n",
    "-->Accuracy is almost the same for 10,50, and 100 epochs. It slightly increases with number of epochs.\n",
    "-->Training for 50,100 epochs wastes time without major improvement.\n",
    "\n",
    "*COMBINATION REVIEW:*\n",
    "-->Batch Size:100, Epochs:10 is the best with almost the same accuarcy and faster execution.\n",
    "-->Combinations with Batch Size:1 are the worst just increases the training too high.\n",
    "-->Batch Size:10 combinations are better than 1 but still takes more time than Batch size 100.\n",
    "\n",
    "*GPU Vs CPU:*\n",
    "-->CPU takes too much training time (Not advisable even for single combination with Batch size 1 and 10)\n",
    "-->GPU is faster than cpu\n",
    "-->Gpu takes less time for batch size 10,100 but takes more training time for batch size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fab6e1-f28a-425b-ba97-2740e59761ff",
   "metadata": {},
   "source": [
    "**The results are saved to another file and provided in the same folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ba6b27-851a-4613-9004-6dc3830cace5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
