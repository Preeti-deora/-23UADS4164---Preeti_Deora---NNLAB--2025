{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Objective:**\n",
        "\n",
        "The primary goal of this project is to train a simple fully connected neural network (ANN) on the MNIST dataset and analyze how different batch sizes and epoch values impact performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Description Of Model:**\n",
        "\n",
        "1. Input: Flattened 28×28 images (784 features).\n",
        "2. Hidden Layer: 256 neurons, ReLU activation.\n",
        "3. Output Layer: 10 neurons (digits 0-9), softmax activation.\n",
        "4. Loss Function: Categorical cross-entropy.\n",
        "5. Optimizer: Adam.\n",
        "6. learning rate = 0.1.\n",
        "7. Different batch sizes: 1, 10, 100\n",
        "8. Different epochs: 10, 50, 100\n",
        "9. Metrics: Train loss, train accuracy, test accuracy, confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ngiaiwX8oJl",
        "outputId": "19c1d4e4-46fc-42ba-a649-c8551fe31bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n",
            "Batch Size: 10, Epochs: 100\n",
            "Test Accuracy: 0.3021\n",
            "Execution Time: 1236.24 seconds\n",
            "\n",
            "Batch Size: 10, Epochs: 50\n",
            "Test Accuracy: 0.2982\n",
            "Execution Time: 602.06 seconds\n",
            "\n",
            "Batch Size: 10, Epochs: 10\n",
            "Test Accuracy: 0.2984\n",
            "Execution Time: 122.82 seconds\n",
            "\n",
            "Batch Size: 100, Epochs: 100\n",
            "Test Accuracy: 0.3281\n",
            "Execution Time: 136.75 seconds\n",
            "\n",
            "Batch Size: 100, Epochs: 50\n",
            "Test Accuracy: 0.3284\n",
            "Execution Time: 69.26 seconds\n",
            "\n",
            "Batch Size: 100, Epochs: 10\n",
            "Test Accuracy: 0.3305\n",
            "Execution Time: 13.85 seconds\n",
            "\n",
            "Results saved to mnist_nn_results.xlsx\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "import os\n",
        "from openpyxl import Workbook, load_workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('GPU not found. Using CPU instead.')\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = tf.convert_to_tensor(x_train.reshape(-1, 784).astype(np.float32) / 255.0)\n",
        "x_test = tf.convert_to_tensor(x_test.reshape(-1, 784).astype(np.float32) / 255.0)\n",
        "\n",
        "y_train = tf.one_hot(y_train, depth=10, dtype=tf.float32)\n",
        "y_test_labels = tf.convert_to_tensor(y_test)  # Original labels for confusion matrix\n",
        "y_test = tf.one_hot(y_test, depth=10, dtype=tf.float32)\n",
        "\n",
        "# Model parameters\n",
        "n_hidden = 256\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "W1 = tf.Variable(tf.random.normal([784, n_hidden], stddev=0.1, dtype=tf.float32))\n",
        "b1 = tf.Variable(tf.random.normal([n_hidden], stddev=0.1, dtype=tf.float32))\n",
        "W2 = tf.Variable(tf.random.normal([n_hidden, 10], stddev=0.1, dtype=tf.float32))\n",
        "b2 = tf.Variable(tf.random.normal([10], stddev=0.1, dtype=tf.float32))\n",
        "\n",
        "# Define model\n",
        "@tf.function\n",
        "def model(X):\n",
        "    Z1 = tf.add(tf.matmul(X, W1), b1)\n",
        "    A1 = tf.nn.relu(Z1)\n",
        "    Z2 = tf.add(tf.matmul(A1, W2), b2)\n",
        "    return Z2\n",
        "\n",
        "# Loss function\n",
        "def loss_fn(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = tf.optimizers.Adam(learning_rate=0.1)\n",
        "\n",
        "# Training step\n",
        "@tf.function\n",
        "def train_step(batch_x, batch_y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(batch_x)\n",
        "        loss = loss_fn(batch_y, logits)\n",
        "    grads = tape.gradient(loss, [W1, b1, W2, b2])\n",
        "    optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2]))\n",
        "    return loss\n",
        "\n",
        "# Configurations for batch size and epochs\n",
        "configs = [(10, 100), (10, 50), (10, 10), (100, 100), (100, 50), (100, 10), (1, 100), (1, 50), (1, 10)]\n",
        "\n",
        "# Excel file setup\n",
        "output_file = \"mnist_nn_results.xlsx\"\n",
        "\n",
        "# If file exists, load; else, create a new one\n",
        "if os.path.exists(output_file):\n",
        "    try:\n",
        "        wb = load_workbook(output_file)\n",
        "        ws = wb.active\n",
        "    except:\n",
        "        os.remove(output_file)  # Delete corrupted file\n",
        "        wb = Workbook()\n",
        "        ws = wb.active\n",
        "        ws.title = \"Results\"\n",
        "        ws.append([\"Batch Size\", \"Epochs\", \"Train Loss\", \"Train Accuracy\", \"Test Accuracy\", \"Execution Time\"])\n",
        "        wb.save(output_file)\n",
        "else:\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    ws.title = \"Results\"\n",
        "    ws.append([\"Batch Size\", \"Epochs\", \"Train Loss\", \"Train Accuracy\", \"Test Accuracy\", \"Execution Time\"])\n",
        "    wb.save(output_file)\n",
        "\n",
        "# Run training\n",
        "with tf.device('/GPU:0'):\n",
        "    for batch_size, epochs in configs:\n",
        "        start_time = time.time()\n",
        "        loss_curve, acc_curve = [], []\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size).shuffle(10000).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for batch_x, batch_y in dataset:\n",
        "                train_loss = train_step(batch_x, batch_y)\n",
        "\n",
        "            # Compute train accuracy\n",
        "            train_logits = model(x_train)\n",
        "            train_acc = tf.reduce_mean(tf.cast(tf.argmax(train_logits, axis=1) == tf.argmax(y_train, axis=1), tf.float32))\n",
        "            loss_curve.append(train_loss.numpy())\n",
        "            acc_curve.append(train_acc.numpy())\n",
        "\n",
        "        # Compute test accuracy\n",
        "        test_logits = model(x_test)\n",
        "        test_acc = tf.reduce_mean(tf.cast(tf.argmax(test_logits, axis=1) == tf.argmax(y_test, axis=1), tf.float32))\n",
        "        y_pred = tf.argmax(test_logits, axis=1)\n",
        "        conf_matrix = confusion_matrix(y_test_labels.numpy(), y_pred.numpy())\n",
        "        exec_time = time.time() - start_time\n",
        "\n",
        "        # Append results to Excel\n",
        "        ws.append([batch_size, epochs, train_loss.numpy(), train_acc.numpy(), test_acc.numpy(), exec_time])\n",
        "\n",
        "        # Save intermediate results\n",
        "        wb.save(output_file)\n",
        "\n",
        "        # Plot Loss and Accuracy Curves\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(epochs), loss_curve, label='Loss', color='red')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(f'Loss Curve (Batch {batch_size}, Epochs {epochs})')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(range(epochs), acc_curve, label='Accuracy', color='blue')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title(f'Accuracy Curve (Batch {batch_size}, Epochs {epochs})')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"mnist_curves_{batch_size}_{epochs}.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Save confusion matrix\n",
        "        df_conf_matrix = pd.DataFrame(conf_matrix)\n",
        "        with pd.ExcelWriter(output_file, mode='a', if_sheet_exists=\"new\") as writer:\n",
        "            df_conf_matrix.to_excel(writer, sheet_name=f\"Conf_Matrix_{batch_size}_{epochs}\")\n",
        "\n",
        "        # Add accuracy/loss curve image to Excel\n",
        "        ws_image = wb.create_sheet(title=f\"Curve_{batch_size}_{epochs}\")\n",
        "        img = Image(f\"mnist_curves_{batch_size}_{epochs}.png\")\n",
        "        ws_image.add_image(img, \"A1\")\n",
        "\n",
        "        print(f\"Batch Size: {batch_size}, Epochs: {epochs}\")\n",
        "        print(f\"Test Accuracy: {test_acc.numpy():.4f}\")\n",
        "        print(f\"Execution Time: {exec_time:.2f} seconds\\n\")\n",
        "\n",
        "    # Final Save\n",
        "    wb.save(output_file)\n",
        "    wb.close()\n",
        "\n",
        "print(f\"Results saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Description Of Code**\n",
        "\n",
        "\n",
        "**1. Data Preprocessing**\n",
        "\n",
        "   11. The MNIST dataset is loaded and split into training (x_train, y_train) and test (x_test, y_test) sets.\n",
        "   12. Images are reshaped from (28,28) to (784,) and normalized to values between 0 and 1.\n",
        "   13. Labels are converted to one-hot encoded vectors for categorical classification.\n",
        "\n",
        "\n",
        "**2. Model Initialization**\n",
        "\n",
        "   21. Weights (W1, W2) and biases (b1, b2) are initialized for both layers using a small random normal distribution.\n",
        "\n",
        "\n",
        "**3. Forward Propagation (model())**\n",
        "\n",
        "   31. Layer 1: Z1 = X × W1 + b1, followed by ReLU(Z1).\n",
        "   32. Output Layer: Z2 = A1 × W2 + b2, which gives logits (pre-softmax scores).\n",
        "\n",
        "\n",
        "**4. Loss Calculation (loss_fn())**\n",
        "\n",
        "   41. Computes categorical cross-entropy loss between predicted logits and true labels.\n",
        "\n",
        "\n",
        "**5. Training Step (train_step())**\n",
        "\n",
        "   51. Uses Gradient Tape to compute gradients and updates weights via Adam optimizer.\n",
        "\n",
        "\n",
        "**6. Training Loop**\n",
        "\n",
        "   61. Iterates over multiple epochs, updating weights and tracking loss and accuracy.\n",
        "   62. Runs for different batch size-epoch configurations, as listed below:\n",
        "     (10, 100), (10, 50), (10, 10)\n",
        "     (100, 100), (100, 50), (100, 10)\n",
        "     (1, 100), (1, 50), (1, 10)\n",
        "\n",
        "\n",
        "**7. Performance Evaluation**\n",
        "\n",
        "   71. Computes training accuracy at each epoch.\n",
        "   72. Computes test accuracy after training completion.\n",
        "   73. Generates confusion matrices to visualize prediction errors.\n",
        "   74. Saves loss and accuracy curves for better visualization.\n",
        "\n",
        "\n",
        "**8. Saving Results to Excel**\n",
        "\n",
        "   81. Stores results such as batch size, epochs, train loss, train accuracy, test accuracy, and execution time in an Excel file (mnist_nn_results.xlsx).\n",
        "   82. Loss and accuracy plots are saved as images and embedded into the Excel file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations from Results:**\n",
        "\n",
        "1. Best Train & Test Accuracy: 32.9% (Batch Size: 100, Epochs: 100).\n",
        "2. Training with Batch Size = 1 took more time but did not improve accuracy.\n",
        "3. Increasing epochs improves accuracy slightly, but beyond 50 epochs, improvements diminish.\n",
        "4. Execution time is much lower for larger batch sizes.\n",
        "5. Test accuracy remains around 32%, indicating an issue with learning stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**My Comments:**\n",
        "\n",
        "1. Reducing  learning rate (e.g., 0.01 or 0.001) can improve accuracy.\n",
        "2. Increasing the number of hidden layers may improve model accuracy.\n",
        "3. In current model the use of batch size 1 is not required and just increasing the training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
